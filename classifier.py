import torch
from torch import nn
from transformers import BertModel


class BertClassifierParallel(nn.Module):
    """The classifier model. BERT with a classification output head.
  """

    def __init__(self, hidden, model_type, dropout=0.2, sigma=True):
        """ Create the BertClassifier.
        Params:
          - hidden: the size of the hidden layer
          - model_type: the name of the BERT model from HuggingFace
          - droupout: rate of the dropout to be applied
    """
        super(BertClassifierParallel, self).__init__()
        self.bert = BertModel.from_pretrained(model_type, output_attentions=True)
        self.parallel = nn.Conv1d(1, 1, 5)
        self.pool = nn.MaxPool1d(10)
        self.dropout = nn.Dropout(dropout)
        self.relu = nn.ReLU()
        self.linear1 = nn.Linear(hidden + 24, 25)
        # self.linear2 = nn.Linear(50, 25)
        self.linear3 = nn.Linear(25, 1)
        if sigma:
            self.activation = nn.Sigmoid()
        self.sigma = sigma

    def forward(self, input_id, mask, text):
        """ The forward pass of the BERT classifier.
       Params:
          - input_id: input_id from the tokenizer
          - mask: mask generated by the tokenizer
       Returns:
          - final_layer: the final output for the classification purpose
          - bert_outputs: all the outputs of the BERT model (together with attention)
    """
        bert_outputs = self.bert(input_ids=input_id, attention_mask=mask)
        parallel_output = self.parallel(text)
        pooled_output = bert_outputs['pooler_output']
        pp_output = self.pool(parallel_output)
        combined_layer = torch.cat([pooled_output, pp_output.squeeze(1)], dim=1)
        hidden_layer1 = self.linear1(self.dropout(self.relu(combined_layer)))
        # hidden_layer2 = self.linear2(self.dropout(self.relu(hidden_layer1)))
        hidden_layer3 = self.linear3(self.dropout(self.relu(hidden_layer1)))
        if self.sigma:
            final_layer = self.activation(hidden_layer3)
            return final_layer, bert_outputs
        else:
            return hidden_layer3, bert_outputs


class BertClassifier10(nn.Module):
    """The classifier model. BERT with a classification output head.
  """

    def __init__(self, hidden, model_type, dropout=0.2, sigma=True):
        """ Create the BertClassifier.
        Params:
          - hidden: the size of the hidden layer
          - model_type: the name of the BERT model from HuggingFace
          - droupout: rate of the dropout to be applied
    """
        super(BertClassifier10, self).__init__()
        self.bert = BertModel.from_pretrained(model_type, output_attentions=True)
        # self.bert.train()
        self.dropout = nn.Dropout(dropout)
        self.relu = nn.ReLU()
        self.linear1 = nn.Linear(hidden, 10)
        # self.linear2 = nn.Linear(50, 25)
        self.linear3 = nn.Linear(10, 1)
        if sigma:
            self.activation = nn.Sigmoid()
        self.sigma = sigma

    def forward(self, input_id, mask):
        """ The forward pass of the BERT classifier.
       Params:
          - input_id: input_id from the tokenizer
          - mask: mask generated by the tokenizer
       Returns:
          - final_layer: the final output for the classification purpose
          - bert_outputs: all the outputs of the BERT model (together with attention)
    """
        bert_outputs = self.bert(input_ids=input_id, attention_mask=mask)
        pooled_output = bert_outputs['pooler_output']
        hidden_layer1 = self.linear1(self.dropout(self.relu(pooled_output)))
        # hidden_layer2 = self.linear2(self.dropout(self.relu(hidden_layer1)))
        hidden_layer3 = self.linear3(self.dropout(self.relu(hidden_layer1)))
        if self.sigma:
            final_layer = self.activation(hidden_layer3)
            return final_layer, bert_outputs
        else:
            return hidden_layer3, bert_outputs


class BertClassifierConv(nn.Module):
    """The classifier model. BERT with a classification output head.
  """

    def __init__(self, hidden, model_type, dropout=0.2, sigma=True):

        super(BertClassifierConv, self).__init__()
        self.relu = nn.ReLU()

        self.bert = BertModel.from_pretrained(model_type, output_attentions=True)
        self.dropBert = nn.Dropout(dropout)

        self.convolution = nn.Conv1d(1, 10, 5)
        conv_size = hidden - 5 + 1

        self.pool = nn.MaxPool1d(conv_size)
        self.flatten = nn.Flatten()

        self.linear1 = nn.Linear(10, 5)
        self.dropout1 = nn.Dropout(dropout)

        self.linear3 = nn.Linear(5, 1)

        if sigma:
            self.activation = nn.Sigmoid()
        self.sigma = sigma

    def forward(self, input_id, mask):

        bert_outputs = self.bert(input_ids=input_id, attention_mask=mask)
        pooled_output = bert_outputs['pooler_output']
        pooled_output = self.dropBert(self.relu(pooled_output.unsqueeze(1)))

        conv_layer = self.convolution(pooled_output)
        pool_layer = self.pool(self.relu(conv_layer))
        flat_layer = self.flatten(pool_layer)

        hidden_layer1 = self.linear1(flat_layer)
        hidden_layer1 = self.dropout1(self.relu(hidden_layer1))

        hidden_layer3 = self.linear3(hidden_layer1)

        if self.sigma:
            final_layer = self.activation(hidden_layer3)
            return final_layer, bert_outputs
        else:
            return hidden_layer3, bert_outputs


class BertClassifier25(nn.Module):
    """The classifier model. BERT with a classification output head.
  """

    def __init__(self, hidden, model_type, dropout=0.2, sigma=True):
        """ Create the BertClassifier.
        Params:
          - hidden: the size of the hidden layer
          - model_type: the name of the BERT model from HuggingFace
          - droupout: rate of the dropout to be applied
    """
        super(BertClassifier25, self).__init__()
        self.bert = BertModel.from_pretrained(model_type, output_attentions=True)
        # self.bert.train()
        self.dropout = nn.Dropout(dropout)
        self.relu = nn.ReLU()
        self.linear1 = nn.Linear(hidden, 25)
        # self.linear2 = nn.Linear(50, 25)
        self.linear3 = nn.Linear(25, 1)
        if sigma:
            self.activation = nn.Sigmoid()
        self.sigma = sigma

    def forward(self, input_id, mask):
        """ The forward pass of the BERT classifier.
       Params:
          - input_id: input_id from the tokenizer
          - mask: mask generated by the tokenizer
       Returns:
          - final_layer: the final output for the classification purpose
          - bert_outputs: all the outputs of the BERT model (together with attention)
    """
        bert_outputs = self.bert(input_ids=input_id, attention_mask=mask)
        pooled_output = bert_outputs['pooler_output']
        hidden_layer1 = self.linear1(self.dropout(self.relu(pooled_output)))
        # hidden_layer2 = self.linear2(self.dropout(self.relu(hidden_layer1)))
        hidden_layer3 = self.linear3(self.dropout(self.relu(hidden_layer1)))
        if self.sigma:
            final_layer = self.activation(hidden_layer3)
            return final_layer, bert_outputs
        else:
            return hidden_layer3, bert_outputs


class BertClassifier50(nn.Module):
    """The classifier model. BERT with a classification output head.
  """

    def __init__(self, hidden, model_type, dropout=0.2, sigma=True):
        """ Create the BertClassifier.
        Params:
          - hidden: the size of the hidden layer
          - model_type: the name of the BERT model from HuggingFace
          - droupout: rate of the dropout to be applied
    """
        super(BertClassifier50, self).__init__()
        self.bert = BertModel.from_pretrained(model_type, output_attentions=True)
        # self.bert.train()
        self.dropout = nn.Dropout(dropout)
        self.relu = nn.ReLU()
        self.linear1 = nn.Linear(hidden, 50)
        # self.linear2 = nn.Linear(50, 25)
        self.linear3 = nn.Linear(50, 1)
        if sigma:
            self.activation = nn.Sigmoid()
        self.sigma = sigma

    def forward(self, input_id, mask):
        """ The forward pass of the BERT classifier.
       Params:
          - input_id: input_id from the tokenizer
          - mask: mask generated by the tokenizer
       Returns:
          - final_layer: the final output for the classification purpose
          - bert_outputs: all the outputs of the BERT model (together with attention)
    """
        bert_outputs = self.bert(input_ids=input_id, attention_mask=mask)
        pooled_output = bert_outputs['pooler_output']
        hidden_layer1 = self.linear1(self.dropout(self.relu(pooled_output)))
        # hidden_layer2 = self.linear2(self.dropout(self.relu(hidden_layer1)))
        hidden_layer3 = self.linear3(self.dropout(self.relu(hidden_layer1)))
        if self.sigma:
            final_layer = self.activation(hidden_layer3)
            return final_layer, bert_outputs
        else:
            return hidden_layer3, bert_outputs


class BertClassifier5025(nn.Module):
    """The classifier model. BERT with a classification output head.
  """

    def __init__(self, hidden, model_type, dropout=0.2, sigma=True):
        """ Create the BertClassifier.
        Params:
          - hidden: the size of the hidden layer
          - model_type: the name of the BERT model from HuggingFace
          - droupout: rate of the dropout to be applied
    """
        super(BertClassifier5025, self).__init__()
        self.bert = BertModel.from_pretrained(model_type, output_attentions=True)
        # self.bert.train()
        self.dropout = nn.Dropout(dropout)
        self.relu = nn.ReLU()
        self.linear1 = nn.Linear(hidden, 50)
        self.linear2 = nn.Linear(50, 25)
        self.linear3 = nn.Linear(25, 1)
        if sigma:
            self.activation = nn.Sigmoid()
        self.sigma = sigma

    def forward(self, input_id, mask):
        """ The forward pass of the BERT classifier.
       Params:
          - input_id: input_id from the tokenizer
          - mask: mask generated by the tokenizer
       Returns:
          - final_layer: the final output for the classification purpose
          - bert_outputs: all the outputs of the BERT model (together with attention)
    """
        bert_outputs = self.bert(input_ids=input_id, attention_mask=mask)
        pooled_output = bert_outputs['pooler_output']
        hidden_layer1 = self.linear1(self.dropout(self.relu(pooled_output)))
        hidden_layer2 = self.linear2(self.dropout(self.relu(hidden_layer1)))
        hidden_layer3 = self.linear3(self.dropout(self.relu(hidden_layer2)))
        if self.sigma:
            final_layer = self.activation(hidden_layer3)
            return final_layer, bert_outputs
        else:
            return hidden_layer3, bert_outputs


class BertClassifier2525(nn.Module):
    """The classifier model. BERT with a classification output head.
  """

    def __init__(self, hidden, model_type, dropout=0.2, sigma=True):
        """ Create the BertClassifier.
        Params:
          - hidden: the size of the hidden layer
          - model_type: the name of the BERT model from HuggingFace
          - droupout: rate of the dropout to be applied
    """
        super(BertClassifier2525, self).__init__()
        self.bert = BertModel.from_pretrained(model_type, output_attentions=True)
        # self.bert.train()
        self.dropout = nn.Dropout(dropout)
        self.relu = nn.ReLU()
        self.linear1 = nn.Linear(hidden, 25)
        self.linear2 = nn.Linear(25, 25)
        self.linear3 = nn.Linear(25, 1)
        if sigma:
            self.activation = nn.Sigmoid()
        self.sigma = sigma

    def forward(self, input_id, mask):
        """ The forward pass of the BERT classifier.
       Params:
          - input_id: input_id from the tokenizer
          - mask: mask generated by the tokenizer
       Returns:
          - final_layer: the final output for the classification purpose
          - bert_outputs: all the outputs of the BERT model (together with attention)
    """
        bert_outputs = self.bert(input_ids=input_id, attention_mask=mask)
        pooled_output = bert_outputs['pooler_output']
        hidden_layer1 = self.linear1(self.dropout(self.relu(pooled_output)))
        hidden_layer2 = self.linear2(self.dropout(self.relu(hidden_layer1)))
        hidden_layer3 = self.linear3(self.dropout(self.relu(hidden_layer2)))
        if self.sigma:
            final_layer = self.activation(hidden_layer3)
            return final_layer, bert_outputs
        else:
            return hidden_layer3, bert_outputs
